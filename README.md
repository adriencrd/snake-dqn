# üêç Snake + Deep Q-Network (DQN)

Projet d'apprentissage par renforcement o√π un agent apprend √† jouer au Snake en utilisant l'algorithme DQN.

## üìã Table des mati√®res

- [Installation](#-installation)
- [Structure du projet](#-structure-du-projet)
- [Commandes principales](#-commandes-principales)
- [Configuration](#-configuration)
- [Comment √ßa marche](#-comment-√ßa-marche)
- [R√©sultats attendus](#-r√©sultats-attendus)
- [Troubleshooting](#-troubleshooting)

---

## üîß Installation

### Pr√©requis
- Python 3.8 ou sup√©rieur
- pip

### √âtape 1 : Cloner ou t√©l√©charger le projet

```bash
cd snake-dqn
```

### √âtape 2 : Installer les d√©pendances

```bash
pip install torch numpy pygame
```

**Ou avec un fichier requirements.txt :**

```bash
pip install -r requirements.txt
```

**Contenu du requirements.txt :**
```
torch>=2.0.0
numpy>=1.24.0
pygame>=2.5.0
```

---

## üìÅ Structure du projet

```
snake-dqn/
‚îÇ
‚îú‚îÄ‚îÄ snake_game.py          # Environnement du jeu Snake
‚îú‚îÄ‚îÄ dqn_agent.py           # Agent DQN avec r√©seau de neurones
‚îú‚îÄ‚îÄ main.py                # Script d'entra√Ænement et de test
‚îú‚îÄ‚îÄ jouer_manuel.py        # Mode jeu manuel
‚îú‚îÄ‚îÄ README.md              # Cette documentation
‚îÇ
‚îî‚îÄ‚îÄ (apr√®s entra√Ænement)
    ‚îú‚îÄ‚îÄ meilleur_modele.pth    # Meilleur mod√®le sauvegard√©
    ‚îî‚îÄ‚îÄ modele_final.pth       # Mod√®le final
```

---

## üéÆ Commandes principales

### 1. Jouer manuellement

Pour tester le jeu et comprendre la m√©canique :

```bash
python jouer_manuel.py
```

**Contr√¥les :**
- `‚Üê` Fl√®che gauche : Tourner √† gauche
- `‚Üí` Fl√®che droite : Tourner √† droite
- `‚Üë` ou `‚Üì` : Continuer tout droit
- `ESPACE` : Red√©marrer la partie
- `ESC` : Quitter

---

### 2. Entra√Æner l'agent DQN

#### Mode standard (sans affichage, rapide)

```bash
python main.py
```

**Avantages :**
- Entra√Ænement rapide
- Pas de ralentissement graphique
- Id√©al pour les longues sessions

#### Mode avec affichage (voir l'agent apprendre)

```bash
python main.py --display
```

**Avantages :**
- Visualiser l'apprentissage en temps r√©el
- Comprendre les strat√©gies de l'agent
- Plus lent mais √©ducatif

**Param√®tres par d√©faut :**
- 1000 √©pisodes d'entra√Ænement
- Sauvegarde automatique du meilleur mod√®le
- Affichage des statistiques tous les 10 √©pisodes

---

### 3. Tester l'agent entra√Æn√©

```bash
python main.py test
```

**Ce qui se passe :**
- Charge le mod√®le `meilleur_modele.pth`
- Joue 10 parties en mode greedy (sans exploration)
- Affiche les statistiques finales

---

## ‚öôÔ∏è Configuration

### Modifier les param√®tres du jeu

Dans `main.py`, modifiez `ConfigJeu` :

```python
config_jeu = ConfigJeu(
    taille_grille=20,      # Taille de la grille (20x20)
    taille_case=32,        # Taille d'une case en pixels
    fps=15,                # Images par seconde
    max_etapes=500,        # Limite d'√©tapes par √©pisode
    recompense_nourriture=1.0,    # R√©compense pour manger
    recompense_mort=-1.0,          # P√©nalit√© pour mourir
    recompense_deplacement=-0.01   # P√©nalit√© par mouvement
)
```

### Modifier les param√®tres DQN

Dans `main.py`, modifiez `ConfigDQN` :

```python
config_dqn = ConfigDQN(
    gamma=0.99,              # Facteur de discount
    lr=1e-3,                 # Taux d'apprentissage
    batch_size=64,           # Taille du batch
    buffer_size=100_000,     # Taille du replay buffer
    debut_apprentissage=1_000,    # √âtapes avant d'apprendre
    freq_entrainement=1,     # Fr√©quence d'entra√Ænement
    sync_target=1000,        # Sync target network
    epsilon_debut=1.0,       # Exploration initiale (100%)
    epsilon_fin=0.05,        # Exploration finale (5%)
    epsilon_decay=20_000     # Decay sur 20k √©tapes
)
```

### Modifier le nombre d'√©pisodes

```python
entrainer(nb_episodes=2000, affichage=False)  # 2000 √©pisodes
```

---

## üß† Comment √ßa marche

### Architecture

#### 1. **Environnement (snake_game.py)**
- Grille NxN
- 3 actions : avancer, tourner gauche, tourner droite
- √âtat : vecteur de 11 dimensions
  - 3 dangers (devant, gauche, droite)
  - 4 directions (haut, bas, gauche, droite)
  - 4 positions nourriture (relative)

#### 2. **Agent DQN (dqn_agent.py)**
- R√©seau de neurones : 11 ‚Üí 128 ‚Üí 128 ‚Üí 3
- Replay buffer pour stocker les exp√©riences
- Target network pour stabiliser l'apprentissage
- Epsilon-greedy pour l'exploration

#### 3. **Boucle d'entra√Ænement**
```
Pour chaque √©pisode :
    1. Reset l'environnement
    2. Tant que non termin√© :
        - Choisir action (epsilon-greedy)
        - Ex√©cuter action
        - M√©moriser transition
        - Apprendre du batch
        - Synchroniser target network
    3. Sauvegarder si meilleur score
```

---

## üìä R√©sultats attendus

### Progression typique

| √âpisodes | Taille serpent | Epsilon | Comportement |
|----------|----------------|---------|--------------|
| 0-100    | 2-3            | 1.0-0.8 | Exploration al√©atoire |
| 100-500  | 3-5            | 0.8-0.3 | Commence √† apprendre |
| 500-1000 | 5-10           | 0.3-0.1 | Strat√©gies √©mergentes |
| 1000+    | 10-20+         | 0.05    | Bon joueur |

### Fichiers g√©n√©r√©s

```
meilleur_modele.pth    # Meilleur score pendant l'entra√Ænement
modele_final.pth       # √âtat final apr√®s tous les √©pisodes
```

### Statistiques affich√©es

```
√âpisode 100/1000
  Score moyen (10 derniers): 4.2
  Meilleur score: 8
  Epsilon: 0.368
  Perte: 0.0234
  Buffer: 15420
```

---

## üîç Troubleshooting

### Erreur : "ModuleNotFoundError: No module named 'pygame'"

**Solution :**
```bash
pip install pygame
```

### Erreur : "ModuleNotFoundError: No module named 'torch'"

**Solution :**
```bash
pip install torch
```

**Ou pour CPU uniquement :**
```bash
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

### L'agent n'apprend pas / stagne

**Solutions possibles :**

1. **Augmenter les √©pisodes :**
```python
entrainer(nb_episodes=2000)
```

2. **R√©duire epsilon_decay :**
```python
config_dqn = ConfigDQN(epsilon_decay=10_000)  # Plus lent
```

3. **Ajuster le learning rate :**
```python
config_dqn = ConfigDQN(lr=5e-4)  # Plus petit
```

### Pygame ne s'affiche pas

**Sur Linux :**
```bash
sudo apt-get install python3-pygame
```

**Sur macOS :**
```bash
brew install sdl sdl_image sdl_mixer sdl_ttf portmidi
pip install pygame
```

### CUDA out of memory

**Solution :** Le code utilise automatiquement CPU si CUDA n'est pas disponible.

Pour forcer le CPU :
```python
self.device = torch.device("cpu")  # Dans dqn_agent.py
```

---

## üöÄ Commandes avanc√©es

### Entra√Ænement personnalis√©

Cr√©ez votre propre script :

```python
from snake_game import SnakeEnv, ConfigJeu
from dqn_agent import AgentDQN, ConfigDQN

# Configuration personnalis√©e
config_jeu = ConfigJeu(taille_grille=30)
config_dqn = ConfigDQN(lr=5e-4, epsilon_decay=50_000)

env = SnakeEnv(config_jeu)
agent = AgentDQN(dim_etat=11, nb_actions=3, config=config_dqn)

# Votre boucle d'entra√Ænement...
```

### Charger et continuer l'entra√Ænement

```python
agent.charger('meilleur_modele.pth')
# Continue l'entra√Ænement...
```

### √âvaluer sans affichage

Modifiez la fonction `tester()` dans main.py :

```python
env = SnakeEnv(config_jeu, affichage=False)  # Pas d'affichage
```

---

## üìà Am√©lioration du mod√®le

### Id√©es pour am√©liorer l'agent

1. **Double DQN** : R√©duire la surestimation des Q-values
2. **Dueling DQN** : S√©parer V(s) et A(s,a)
3. **Prioritized Experience Replay** : √âchantillonner les transitions importantes
4. **Reward shaping** : Ajouter des r√©compenses interm√©diaires
5. **Curriculum learning** : Commencer avec une petite grille

---

## üìù Licence

Projet √©ducatif - Libre d'utilisation et de modification

---

## ü§ù Contribution

N'h√©sitez pas √† :
- Exp√©rimenter avec les hyperparam√®tres
- Am√©liorer l'architecture du r√©seau
- Ajouter de nouvelles fonctionnalit√©s
- Partager vos r√©sultats !

---

**Bon entra√Ænement ! üêçüéÆü§ñ**